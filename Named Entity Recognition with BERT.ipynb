{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-17T03:02:10.301208Z","iopub.execute_input":"2022-12-17T03:02:10.301752Z","iopub.status.idle":"2022-12-17T03:02:10.329254Z","shell.execute_reply.started":"2022-12-17T03:02:10.301648Z","shell.execute_reply":"2022-12-17T03:02:10.328219Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\n!pip install evaluate\n!pip install f1","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:02:33.295539Z","iopub.execute_input":"2022-12-17T03:02:33.295887Z","iopub.status.idle":"2022-12-17T03:03:08.256552Z","shell.execute_reply.started":"2022-12-17T03:02:33.295817Z","shell.execute_reply":"2022-12-17T03:03:08.255430Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.8.2)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.13)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.13.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.5.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (5.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting f1\n  Downloading f1-1.0.1.zip (757 bytes)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: f1\n  Building wheel for f1 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for f1: filename=f1-1.0.1-py3-none-any.whl size=1204 sha256=bb7b766a3fbca764cd6428f20314610812abdbbe4f0f999061cb182b74051437\n  Stored in directory: /root/.cache/pip/wheels/5e/34/53/87317fe4016e96aa8f77d61d883fce7b7a5afcf558ed05eb05\nSuccessfully built f1\nInstalling collected packages: f1\nSuccessfully installed f1-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport evaluate \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizerFast,AutoTokenizer,DataCollatorForTokenClassification,AutoModelForTokenClassification, TrainingArguments, Trainer,EarlyStoppingCallback\nimport json","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:04:41.949798Z","iopub.execute_input":"2022-12-17T03:04:41.950426Z","iopub.status.idle":"2022-12-17T03:04:53.797335Z","shell.execute_reply.started":"2022-12-17T03:04:41.950387Z","shell.execute_reply":"2022-12-17T03:04:53.796162Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = os.path.join(os.getcwd(), \"/kaggle/input/dataset\")\n\nwith open(os.path.join(path,'train.json'), 'r') as f:\n     train = json.loads(f.read())\n\nwith open(os.path.join(path,'test.json'), 'r') as f:\n     test = json.loads(f.read())","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:05:05.966093Z","iopub.execute_input":"2022-12-17T03:05:05.967259Z","iopub.status.idle":"2022-12-17T03:05:06.141418Z","shell.execute_reply.started":"2022-12-17T03:05:05.967219Z","shell.execute_reply":"2022-12-17T03:05:06.140306Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(train['text'][1])\nprint(train['index'][1])\nprint(train['POS'][1])\nprint(train['NER'][1])","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:05:33.413840Z","iopub.execute_input":"2022-12-17T03:05:33.414226Z","iopub.status.idle":"2022-12-17T03:05:33.421260Z","shell.execute_reply.started":"2022-12-17T03:05:33.414193Z","shell.execute_reply":"2022-12-17T03:05:33.419797Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['Polish', 'schoolgirl', 'blackmailer', 'wanted', 'textbooks', '.', 'GDANSK', ',', 'Poland', '1996-08-22', 'A', 'Polish', 'schoolgirl', 'blackmailed', 'two', 'women', 'with', 'anonymous', 'letters', 'threatening', 'death', 'and', 'later', 'explained', 'that', 'she', 'needed', 'money', 'for', 'textbooks', ',', 'police', 'said', 'on', 'Thursday', '.', '\"', 'The', '13-year-old', 'girl', 'tried', 'to', 'extract', '60', 'and', '70', 'zlotys', '(', '$', '22', 'and', '$', '26', ')', 'from', 'two', 'residents', 'of', 'Sierakowice', 'by', 'threatening', 'to', 'take', 'their', 'lives', ',', '\"', 'a', 'police', 'spokesman', 'said', 'in', 'the', 'nearby', 'northern', 'city', 'of', 'Gdansk', 'on', 'Thursday', '.', 'He', 'said', 'the', 'women', 'reported', 'the', 'blackmail', 'letters', 'and', 'police', 'caught', 'the', 'girl', 'on', 'Wednesday', 'as', 'she', 'tried', 'to', 'pick', 'up', 'the', 'cash', 'at', 'the', 'Sierakowice', 'railway', 'station', '.', '\"', 'Interviewed', 'in', 'the', 'presence', 'of', 'a', 'psychologist', ',', 'she', 'said', 'she', 'wanted', 'to', 'use', 'the', 'money', 'for', 'school', 'books', 'and', 'clothes', ',', '\"', 'spokesman', 'Kazimierz', 'Socha', 'told', 'Reuters', '.', 'He', 'said', 'the', 'case', 'of', 'the', 'girl', ',', 'from', 'a', 'poor', 'family', 'that', 'had', 'never', 'been', 'in', 'trouble', 'with', 'the', 'law', ',', 'would', 'go', 'before', 'a', 'special', 'court', 'dealing', 'with', 'underage', 'offenders', '.']\n[254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426]\n['JJ', 'NN', 'NN', 'VBD', 'NNS', '.', 'NNP', ',', 'NNP', 'CD', 'DT', 'JJ', 'NN', 'VBN', 'CD', 'NNS', 'IN', 'JJ', 'NNS', 'VBG', 'NN', 'CC', 'RB', 'VBN', 'IN', 'PRP', 'VBD', 'NN', 'IN', 'NNS', ',', 'NN', 'VBD', 'IN', 'NNP', '.', '\"', 'DT', 'JJ', 'NN', 'VBD', 'TO', 'VB', 'CD', 'CC', 'CD', 'NNS', '(', '$', 'CD', 'CC', '$', 'CD', ')', 'IN', 'CD', 'NNS', 'IN', 'NNP', 'IN', 'VBG', 'TO', 'VB', 'PRP$', 'NNS', ',', '\"', 'DT', 'NN', 'NN', 'VBD', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'NNP', '.', 'PRP', 'VBD', 'DT', 'NNS', 'VBD', 'DT', 'NN', 'NNS', 'CC', 'NN', 'VBD', 'DT', 'NN', 'IN', 'NNP', 'IN', 'PRP', 'VBD', 'TO', 'VB', 'RP', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', '.', '\"', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBD', 'PRP', 'VBD', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'NNS', ',', '\"', 'NN', 'NNP', 'NNP', 'VBD', 'NNP', '.', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBD', 'RB', 'VBN', 'IN', 'NN', 'IN', 'DT', 'NN', ',', 'MD', 'VB', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'IN', 'JJ', 'NNS', '.']\n['MISC', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'O', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:05:50.689474Z","iopub.execute_input":"2022-12-17T03:05:50.689847Z","iopub.status.idle":"2022-12-17T03:05:52.861206Z","shell.execute_reply.started":"2022-12-17T03:05:50.689814Z","shell.execute_reply":"2022-12-17T03:05:52.860337Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7151503d22d344d3afa5d153ab77f153"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d82eedd88e4fd8a9dc79d5b2aa9b8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dfbae7d85114d4a932991af9f2b4e17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7c4d3741d1401090920e50101f7df9"}},"metadata":{}}]},{"cell_type":"code","source":"label_name = ['O', 'ORG', 'LOC', 'PER','MISC']\nlabel_to_index = {y: x for x, y in enumerate(label_name)}\nindex_to_label = {x: y for x, y in enumerate(label_name)}","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:06:08.650545Z","iopub.execute_input":"2022-12-17T03:06:08.650915Z","iopub.status.idle":"2022-12-17T03:06:08.656643Z","shell.execute_reply.started":"2022-12-17T03:06:08.650882Z","shell.execute_reply":"2022-12-17T03:06:08.655624Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def indices_text_alignment(a_text_token,a_labels,a_indices):\n  label_alignment = []\n  index_alignment =[]\n  for tokenized_text,labels,indices in zip(a_text_token,a_labels,a_indices):\n      input_tokenized = tokenizer(tokenized_text, padding='max_length', max_length=512, truncation=True, is_split_into_words=True)\n      id_of_word = input_tokenized.word_ids()\n      token_labels_all = True\n      prev_word_index = None\n      id_index = []\n      id_labels = []\n\n      for index_word in id_of_word:\n          if index_word is None:\n            id_labels.append(-100)\n            id_index.append(-100)\n          elif index_word != prev_word_index:\n            id_labels.append(label_to_index[labels[index_word]])\n            id_index.append(indices[index_word])\n          elif token_labels_all == True:\n            id_index.append(indices[index_word]) \n            id_labels.append(label_to_index[labels[index_word]]) \n          else: \n            id_labels.append(-100)\n            id_index.append(-100)\n          \n      prev_word_index = index_word\n\n      label_alignment.append(id_labels)\n      index_alignment.append(id_index)\n  return {'labels': label_alignment,'indices': index_alignment}","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:06:21.145524Z","iopub.execute_input":"2022-12-17T03:06:21.145901Z","iopub.status.idle":"2022-12-17T03:06:21.156206Z","shell.execute_reply.started":"2022-12-17T03:06:21.145868Z","shell.execute_reply":"2022-12-17T03:06:21.155176Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_text = [tokenizer(text,padding='max_length', max_length=512, truncation=True,is_split_into_words=True) for text in train['text']]\ntrain_label_indices =  indices_text_alignment(train['text'], train['NER'],train['index'])\ntrain_label_indices_dataframe = pd.DataFrame.from_dict(train_label_indices)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:07:10.548564Z","iopub.execute_input":"2022-12-17T03:07:10.548966Z","iopub.status.idle":"2022-12-17T03:07:12.475829Z","shell.execute_reply.started":"2022-12-17T03:07:10.548933Z","shell.execute_reply":"2022-12-17T03:07:12.474785Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_text_dataframe = pd.DataFrame.from_dict(train_text)\ntrain_dataframe = train_text_dataframe.join(train_label_indices_dataframe)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:07:14.996001Z","iopub.execute_input":"2022-12-17T03:07:14.996366Z","iopub.status.idle":"2022-12-17T03:07:15.016348Z","shell.execute_reply.started":"2022-12-17T03:07:14.996335Z","shell.execute_reply":"2022-12-17T03:07:15.015262Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:07:27.754089Z","iopub.execute_input":"2022-12-17T03:07:27.754450Z","iopub.status.idle":"2022-12-17T03:07:27.800724Z","shell.execute_reply.started":"2022-12-17T03:07:27.754418Z","shell.execute_reply":"2022-12-17T03:07:27.799589Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                           input_ids  \\\n0  [101, 5726, 1352, 4788, 1177, 7666, 1107, 1340...   \n1  [101, 3129, 1278, 17001, 1602, 14746, 1200, 14...   \n2  [101, 2579, 1311, 3794, 3291, 3741, 2653, 3554...   \n3  [101, 1457, 20797, 1673, 117, 20497, 3923, 116...   \n4  [101, 1960, 5888, 1106, 2939, 1111, 3646, 1695...   \n\n                                      token_type_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                              labels  \\\n0  [-100, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, ...   \n1  [-100, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, ...   \n2  [-100, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, ...   \n3  [-100, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, ...   \n4  [-100, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, ...   \n\n                                             indices  \n0  [-100, 0, 1, 2, 3, 3, 4, 5, 6, 7, 7, 7, 7, 7, ...  \n1  [-100, 254, 255, 255, 256, 256, 256, 257, 258,...  \n2  [-100, 427, 428, 429, 430, 431, 432, 432, 433,...  \n3  [-100, 455, 455, 455, 456, 457, 457, 457, 458,...  \n4  [-100, 587, 588, 589, 590, 591, 592, 593, 594,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attention_mask</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>labels</th>\n      <th>indices</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 5726, 1352, 4788, 1177, 7666, 1107, 1340...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, ...</td>\n      <td>[-100, 0, 1, 2, 3, 3, 4, 5, 6, 7, 7, 7, 7, 7, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 3129, 1278, 17001, 1602, 14746, 1200, 14...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, ...</td>\n      <td>[-100, 254, 255, 255, 256, 256, 256, 257, 258,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 2579, 1311, 3794, 3291, 3741, 2653, 3554...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, ...</td>\n      <td>[-100, 427, 428, 429, 430, 431, 432, 432, 433,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 1457, 20797, 1673, 117, 20497, 3923, 116...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, ...</td>\n      <td>[-100, 455, 455, 455, 456, 457, 457, 457, 458,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 1960, 5888, 1106, 2939, 1111, 3646, 1695...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, ...</td>\n      <td>[-100, 587, 588, 589, 590, 591, 592, 593, 594,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_text_tokenized, validation_text_tokenized = train_test_split(train_dataframe, test_size=0.1, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:07:46.351843Z","iopub.execute_input":"2022-12-17T03:07:46.352201Z","iopub.status.idle":"2022-12-17T03:07:46.360977Z","shell.execute_reply.started":"2022-12-17T03:07:46.352170Z","shell.execute_reply":"2022-12-17T03:07:46.360007Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(train_text_tokenized.shape)\nprint(validation_text_tokenized.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:08:17.113180Z","iopub.execute_input":"2022-12-17T03:08:17.113541Z","iopub.status.idle":"2022-12-17T03:08:17.119902Z","shell.execute_reply.started":"2022-12-17T03:08:17.113511Z","shell.execute_reply":"2022-12-17T03:08:17.118607Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(680, 5)\n(76, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_tokenized_dictionary = train_text_tokenized.to_dict('records')\nvalidation_tokenized_dictionary = validation_text_tokenized.to_dict('records')","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:08:20.080068Z","iopub.execute_input":"2022-12-17T03:08:20.080506Z","iopub.status.idle":"2022-12-17T03:08:20.143108Z","shell.execute_reply.started":"2022-12-17T03:08:20.080467Z","shell.execute_reply":"2022-12-17T03:08:20.140121Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\nmetric  = evaluate.load(\"f1\")\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    \n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [l for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n     \n    true_predictions1 = [j for sub in true_predictions for j in sub]\n    true_labels1 = [j for sub in true_labels for j in sub]\n\n    results = metric.compute(predictions=true_predictions1, references=true_labels1,average=\"macro\")\n    print(results)\n   \n    return results","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:08:32.047958Z","iopub.execute_input":"2022-12-17T03:08:32.048315Z","iopub.status.idle":"2022-12-17T03:08:32.650905Z","shell.execute_reply.started":"2022-12-17T03:08:32.048285Z","shell.execute_reply":"2022-12-17T03:08:32.650000Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b5becf9eb84d89814d63ffca0727b7"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\ntraining_args = TrainingArguments(output_dir='./log_results',\n    num_train_epochs=20,\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,   \n    per_device_eval_batch_size=64,\n    weight_decay=0.01,\n    warmup_steps=500, \n    eval_steps=50,\n    save_steps=100,\n    evaluation_strategy=\"steps\",\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokenized_dictionary,\n    eval_dataset=validation_tokenized_dictionary,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForTokenClassification(tokenizer),\n    compute_metrics=compute_metrics,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience = 6)]\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:08:47.139526Z","iopub.execute_input":"2022-12-17T03:08:47.139911Z","iopub.status.idle":"2022-12-17T03:20:40.557489Z","shell.execute_reply.started":"2022-12-17T03:08:47.139876Z","shell.execute_reply":"2022-12-17T03:20:40.556465Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221b7f6af6e941148906ffb96aba2c63"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 680\n  Num Epochs = 20\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 440\nThe following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [440/440 11:29, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>No log</td>\n      <td>1.141954</td>\n      <td>0.220286</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>No log</td>\n      <td>0.401819</td>\n      <td>0.500637</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>No log</td>\n      <td>0.173900</td>\n      <td>0.732488</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.103008</td>\n      <td>0.886270</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>No log</td>\n      <td>0.081983</td>\n      <td>0.912976</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>No log</td>\n      <td>0.073315</td>\n      <td>0.925173</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>No log</td>\n      <td>0.070797</td>\n      <td>0.937318</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>No log</td>\n      <td>0.068796</td>\n      <td>0.941017</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.22028645983102318}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.5006367539771711}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./log_results/checkpoint-100\nConfiguration saved in ./log_results/checkpoint-100/config.json\nModel weights saved in ./log_results/checkpoint-100/pytorch_model.bin\ntokenizer config file saved in ./log_results/checkpoint-100/tokenizer_config.json\nSpecial tokens file saved in ./log_results/checkpoint-100/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.7324877364435775}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.886269971624786}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./log_results/checkpoint-200\nConfiguration saved in ./log_results/checkpoint-200/config.json\nModel weights saved in ./log_results/checkpoint-200/pytorch_model.bin\ntokenizer config file saved in ./log_results/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in ./log_results/checkpoint-200/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.9129756814211675}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.9251728524205639}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./log_results/checkpoint-300\nConfiguration saved in ./log_results/checkpoint-300/config.json\nModel weights saved in ./log_results/checkpoint-300/pytorch_model.bin\ntokenizer config file saved in ./log_results/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in ./log_results/checkpoint-300/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.9373176936560743}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"name":"stdout","text":"{'f1': 0.9410170786195919}\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./log_results/checkpoint-400\nConfiguration saved in ./log_results/checkpoint-400/config.json\nModel weights saved in ./log_results/checkpoint-400/pytorch_model.bin\ntokenizer config file saved in ./log_results/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in ./log_results/checkpoint-400/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./log_results/checkpoint-400 (score: 0.06879574060440063).\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=440, training_loss=0.33890207464044747, metrics={'train_runtime': 696.9023, 'train_samples_per_second': 19.515, 'train_steps_per_second': 0.631, 'total_flos': 3553732276224000.0, 'train_loss': 0.33890207464044747, 'epoch': 20.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(validation_tokenized_dictionary)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:21:46.670272Z","iopub.execute_input":"2022-12-17T03:21:46.670958Z","iopub.status.idle":"2022-12-17T03:21:48.106624Z","shell.execute_reply.started":"2022-12-17T03:21:46.670919Z","shell.execute_reply":"2022-12-17T03:21:48.105657Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 76\n  Batch size = 128\nThe following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 01:58]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'f1': 0.9410170786195919}\n","output_type":"stream"}]},{"cell_type":"code","source":"def max_out(predictions):\n  index_to_label= []\n  for text in predictions:\n    sen = []\n    for label in text:\n      sen.append(label_name[label.argmax()])\n    index_to_label.append(sen)\n  return index_to_label","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:22:40.132009Z","iopub.execute_input":"2022-12-17T03:22:40.132369Z","iopub.status.idle":"2022-12-17T03:22:40.138160Z","shell.execute_reply.started":"2022-12-17T03:22:40.132337Z","shell.execute_reply":"2022-12-17T03:22:40.137073Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"validation_text_tokenized_dataframe = validation_text_tokenized\nvalidation_text_tokenized_dataframe['Predicted_labels'] = max_out(predictions.predictions)\nvalidation_text_tokenized_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:22:49.208702Z","iopub.execute_input":"2022-12-17T03:22:49.209187Z","iopub.status.idle":"2022-12-17T03:22:49.307286Z","shell.execute_reply.started":"2022-12-17T03:22:49.209134Z","shell.execute_reply":"2022-12-17T03:22:49.306253Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                        attention_mask  \\\n84   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n513  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n12   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n753  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n151  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                             input_ids  \\\n84   [101, 12127, 5478, 1708, 1267, 1376, 2810, 110...   \n513  [101, 156, 9244, 10954, 2069, 118, 12465, 1003...   \n12   [101, 156, 9244, 10954, 2069, 118, 26660, 1592...   \n753  [101, 2124, 26422, 9171, 6628, 1103, 1411, 189...   \n151  [101, 156, 9244, 10954, 2069, 118, 143, 16941,...   \n\n                                        token_type_ids  \\\n84   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n513  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n12   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n753  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n151  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                                labels  \\\n84   [-100, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, ...   \n513  [-100, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 4, ...   \n12   [-100, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, ...   \n753  [-100, 4, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, ...   \n151  [-100, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, ...   \n\n                                               indices  \\\n84   [-100, 17601, 17602, 17602, 17603, 17604, 1760...   \n513  [-100, 108947, 108947, 108947, 108947, 108948,...   \n12   [-100, 2025, 2025, 2025, 2025, 2026, 2027, 202...   \n753  [-100, 161789, 161790, 161791, 161792, 161793,...   \n151  [-100, 31505, 31505, 31505, 31505, 31506, 3150...   \n\n                                      Predicted_labels  \n84   [O, MISC, O, O, O, O, O, O, O, O, O, O, PER, P...  \n513  [O, O, O, O, O, O, ORG, ORG, ORG, ORG, ORG, O,...  \n12   [O, O, O, O, O, O, MISC, MISC, MISC, MISC, MIS...  \n753  [O, MISC, O, O, O, O, O, O, O, LOC, LOC, LOC, ...  \n151  [O, O, O, O, O, O, MISC, MISC, MISC, MISC, O, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attention_mask</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>labels</th>\n      <th>indices</th>\n      <th>Predicted_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>84</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 12127, 5478, 1708, 1267, 1376, 2810, 110...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, ...</td>\n      <td>[-100, 17601, 17602, 17602, 17603, 17604, 1760...</td>\n      <td>[O, MISC, O, O, O, O, O, O, O, O, O, O, PER, P...</td>\n    </tr>\n    <tr>\n      <th>513</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 156, 9244, 10954, 2069, 118, 12465, 1003...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 4, ...</td>\n      <td>[-100, 108947, 108947, 108947, 108947, 108948,...</td>\n      <td>[O, O, O, O, O, O, ORG, ORG, ORG, ORG, ORG, O,...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 156, 9244, 10954, 2069, 118, 26660, 1592...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, ...</td>\n      <td>[-100, 2025, 2025, 2025, 2025, 2026, 2027, 202...</td>\n      <td>[O, O, O, O, O, O, MISC, MISC, MISC, MISC, MIS...</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 2124, 26422, 9171, 6628, 1103, 1411, 189...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 4, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, ...</td>\n      <td>[-100, 161789, 161790, 161791, 161792, 161793,...</td>\n      <td>[O, MISC, O, O, O, O, O, O, O, LOC, LOC, LOC, ...</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 156, 9244, 10954, 2069, 118, 143, 16941,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, ...</td>\n      <td>[-100, 31505, 31505, 31505, 31505, 31506, 3150...</td>\n      <td>[O, O, O, O, O, O, MISC, MISC, MISC, MISC, O, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def indices_alignment(tokenized_text,indices):\n    input_tokenized = tokenizer(tokenized_text, padding='max_length', max_length=512, truncation=True, is_split_into_words=True)\n    id_of_word = input_tokenized.word_ids()\n    token_label_all = True\n    prev_word_index = None\n    id_index = []\n\n    for index_word in id_of_word:\n\n        if index_word is None:\n            id_index.append(-100)\n\n        elif index_word != prev_word_index:\n                id_index.append(indices[index_word])\n\n        elif token_label_all == True:\n                id_index.append(indices[index_word]) \n  \n        else:\n          id_index.append(-100)\n\n        prev_word_index = index_word\n\n    return id_index","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:22:59.843442Z","iopub.execute_input":"2022-12-17T03:22:59.843814Z","iopub.status.idle":"2022-12-17T03:22:59.850934Z","shell.execute_reply.started":"2022-12-17T03:22:59.843782Z","shell.execute_reply":"2022-12-17T03:22:59.849810Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"text_test_data = [tokenizer(text,padding='max_length', max_length=512, truncation=True,is_split_into_words=True) for text in test['text']]\nindices_test = {'indices': [indices_alignment(i,j) for i,j in zip(test['text'], test['index'])]}","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:23:09.463442Z","iopub.execute_input":"2022-12-17T03:23:09.463831Z","iopub.status.idle":"2022-12-17T03:23:09.934262Z","shell.execute_reply.started":"2022-12-17T03:23:09.463800Z","shell.execute_reply":"2022-12-17T03:23:09.933218Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"text_test_dataframe = pd.DataFrame.from_dict(text_test_data)\nindices_test_dataframe = pd.DataFrame.from_dict(indices_test)\ntest_dataframe = text_test_dataframe.join(indices_test_dataframe)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:23:17.954866Z","iopub.execute_input":"2022-12-17T03:23:17.955258Z","iopub.status.idle":"2022-12-17T03:23:17.973044Z","shell.execute_reply.started":"2022-12-17T03:23:17.955217Z","shell.execute_reply":"2022-12-17T03:23:17.971813Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_dictionary = test_dataframe.to_dict('records')","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:23:31.471420Z","iopub.execute_input":"2022-12-17T03:23:31.471788Z","iopub.status.idle":"2022-12-17T03:23:31.477857Z","shell.execute_reply.started":"2022-12-17T03:23:31.471755Z","shell.execute_reply":"2022-12-17T03:23:31.476783Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"test_predictions = trainer.predict(test_dictionary)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:23:42.642833Z","iopub.execute_input":"2022-12-17T03:23:42.643177Z","iopub.status.idle":"2022-12-17T03:23:46.054507Z","shell.execute_reply.started":"2022-12-17T03:23:42.643148Z","shell.execute_reply":"2022-12-17T03:23:46.053549Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 189\n  Batch size = 128\nThe following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: indices. If indices are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntest_final_dataframe = test_dataframe\ntest_final_dataframe['predicted_labes'] = max_out(test_predictions.predictions)\ntest_final_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:23:54.495019Z","iopub.execute_input":"2022-12-17T03:23:54.495388Z","iopub.status.idle":"2022-12-17T03:23:54.575791Z","shell.execute_reply.started":"2022-12-17T03:23:54.495356Z","shell.execute_reply":"2022-12-17T03:23:54.574775Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                           input_ids  \\\n0  [101, 10163, 1105, 2870, 4010, 7270, 117, 1001...   \n1  [101, 156, 9244, 10954, 2069, 118, 159, 2346, ...   \n2  [101, 156, 9244, 10954, 2069, 118, 26546, 2198...   \n3  [101, 1999, 17058, 25464, 1116, 12814, 129, 11...   \n4  [101, 145, 9565, 12649, 26547, 19747, 11780, 1...   \n\n                                      token_type_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                             indices  \\\n0  [-100, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, 9, ...   \n1  [-100, 243, 243, 243, 243, 244, 245, 245, 245,...   \n2  [-100, 477, 477, 477, 477, 478, 479, 479, 479,...   \n3  [-100, 743, 744, 745, 745, 746, 747, 747, 747,...   \n4  [-100, 1004, 1004, 1004, 1005, 1005, 1005, 100...   \n\n                                     predicted_labes  \n0  [O, LOC, O, LOC, O, ORG, O, ORG, O, O, LOC, LO...  \n1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n2  [O, O, O, O, O, O, PER, PER, PER, PER, PER, O,...  \n3  [O, LOC, O, O, O, O, O, O, O, O, O, O, O, O, L...  \n4  [O, O, O, O, O, O, O, O, ORG, ORG, PER, PER, O...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attention_mask</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>indices</th>\n      <th>predicted_labes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 10163, 1105, 2870, 4010, 7270, 117, 1001...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, 9, ...</td>\n      <td>[O, LOC, O, LOC, O, ORG, O, ORG, O, O, LOC, LO...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 156, 9244, 10954, 2069, 118, 159, 2346, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 243, 243, 243, 243, 244, 245, 245, 245,...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 156, 9244, 10954, 2069, 118, 26546, 2198...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 477, 477, 477, 477, 478, 479, 479, 479,...</td>\n      <td>[O, O, O, O, O, O, PER, PER, PER, PER, PER, O,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 1999, 17058, 25464, 1116, 12814, 129, 11...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 743, 744, 745, 745, 746, 747, 747, 747,...</td>\n      <td>[O, LOC, O, O, O, O, O, O, O, O, O, O, O, O, L...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[101, 145, 9565, 12649, 26547, 19747, 11780, 1...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[-100, 1004, 1004, 1004, 1005, 1005, 1005, 100...</td>\n      <td>[O, O, O, O, O, O, O, O, ORG, ORG, PER, PER, O...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x = tokenizer.convert_ids_to_tokens(test_final_dataframe[\"input_ids\"][0])\ntoken_indices =[]\ntoken_labels =[]\nfor ids,lables in zip(test_final_dataframe['indices'].to_numpy(),test_final_dataframe['predicted_labes'].to_numpy()):\n  for i,j  in zip(ids,lables):\n    if not(i in token_indices or i == -100):\n        token_labels.append(j)\n        token_indices.append(i)\n   ","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:24:10.362575Z","iopub.execute_input":"2022-12-17T03:24:10.362930Z","iopub.status.idle":"2022-12-17T03:24:34.488608Z","shell.execute_reply.started":"2022-12-17T03:24:10.362900Z","shell.execute_reply":"2022-12-17T03:24:34.487434Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def format_output_labels(token_labels, token_indices):\n    \"\"\"\n    Returns a dictionary that has the labels (LOC, ORG, MISC or PER) as the keys, \n    with the associated value being the list of entities predicted to be of that key label. \n    Each entity is specified by its starting and ending position indicated in [token_indices].\n\n    Eg. if [token_labels] = [\"ORG\", \"ORG\", \"O\", \"O\", \"ORG\"]\n           [token_indices] = [15, 16, 17, 18, 19]\n        then dictionary returned is \n        {'LOC': [], 'MISC': [], 'ORG': [(15, 16), (19, 19)], 'PER': []}\n\n    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n    :type token_labels: List[String]\n    :parameter token_indices: A list of token indices (taken from the dataset) \n                              corresponding to the labels in [token_labels].\n    :type token_indices: List[int]\n    \"\"\"\n    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n    prev_label = token_labels[0]\n    start = token_indices[0]\n    for idx, label in enumerate(token_labels):\n      if prev_label != label:\n        end = token_indices[idx-1]\n        if prev_label != \"O\":\n            label_dict[prev_label].append((start, end))\n        start = token_indices[idx]\n      prev_label = label\n      if idx == len(token_labels) - 1:\n        if prev_label != \"O\":\n            label_dict[prev_label].append((start, token_indices[idx]))\n    return label_dict","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:24:34.490716Z","iopub.execute_input":"2022-12-17T03:24:34.491392Z","iopub.status.idle":"2022-12-17T03:24:34.499892Z","shell.execute_reply.started":"2022-12-17T03:24:34.491352Z","shell.execute_reply":"2022-12-17T03:24:34.498770Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef mean_f1(y_pred_dict, y_true_dict):\n    \"\"\" \n    Calculates the entity-level mean F1 score given the actual/true and \n    predicted span labels.\n    :parameter y_pred_dict: A dictionary containing predicted labels as keys and the \n                            list of associated span labels as the corresponding\n                            values.\n    :type y_pred_dict: Dict<key [String] : value List[Tuple]>\n    :parameter y_true_dict: A dictionary containing true labels as keys and the \n                            list of associated span labels as the corresponding\n                            values.\n    :type y_true_dict: Dict<key [String] : value List[Tuple]>\n\n    Implementation modified from original by author @shonenkov at\n    https://www.kaggle.com/shonenkov/competition-metrics.\n    \"\"\"\n    F1_lst = []\n    for key in y_true_dict:\n        TP, FN, FP = 0, 0, 0\n        num_correct, num_true = 0, 0\n        preds = y_pred_dict[key]\n        trues = y_true_dict[key]\n        for true in trues:\n            num_true += 1\n            if true in preds:\n                num_correct += 1\n            else:\n                continue\n        num_pred = len(preds)\n        if num_true != 0:\n            if num_pred != 0 and num_correct != 0:\n                R = num_correct / num_true\n                P = num_correct / num_pred\n                F1 = 2*P*R / (P + R)\n            else:\n                F1 = 0      # either no predictions or no correct predictions\n        else:\n            continue\n        F1_lst.append(F1)\n    return np.mean(F1_lst)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:24:38.603191Z","iopub.execute_input":"2022-12-17T03:24:38.603898Z","iopub.status.idle":"2022-12-17T03:24:38.611893Z","shell.execute_reply.started":"2022-12-17T03:24:38.603863Z","shell.execute_reply":"2022-12-17T03:24:38.610712Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import csv\n\ndef create_submission(output_filepath, token_labels, token_inds):\n    \"\"\"\n    :parameter output_filepath: The full path (including file name) of the output file, \n                                with extension .csv\n    :type output_filepath: [String]\n    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n    :type token_labels: List[String]\n    :parameter token_indices: A list of token indices (taken from the dataset) \n                              corresponding to the labels in [token_labels].\n    :type token_indices: List[int]\n    \"\"\"\n    label_dict = format_output_labels(token_labels, token_inds)\n    with open(output_filepath, mode='w') as csv_file:\n        fieldnames = ['Id', 'Predicted']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        writer.writeheader()\n        for key in label_dict:\n            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n            writer.writerow({'Id': key, 'Predicted': p_string})","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:24:48.491143Z","iopub.execute_input":"2022-12-17T03:24:48.491507Z","iopub.status.idle":"2022-12-17T03:24:48.498904Z","shell.execute_reply.started":"2022-12-17T03:24:48.491476Z","shell.execute_reply":"2022-12-17T03:24:48.497888Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(43)\n\n# test_pred_labels = ['PER', 'ORG', 'LOC', 'MISC']\n# test_pred_inds = []\n\n# for idx, ex in enumerate(test['text']):\n#   for i, token in enumerate(ex):\n#     test_pred_labels.append(random.choice(['PER', 'ORG', 'LOC', 'MISC', 'O']))\n#   test_pred_inds += test['index'][idx]\n\n# generate the file with predictions (the predicted_random.csv entry on kaggle)\ncreate_submission('/kaggle/working/' + \"/predicted_test_file_nmc210000.csv\", token_labels, token_indices)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T03:25:29.476693Z","iopub.execute_input":"2022-12-17T03:25:29.477052Z","iopub.status.idle":"2022-12-17T03:25:29.498356Z","shell.execute_reply.started":"2022-12-17T03:25:29.477024Z","shell.execute_reply":"2022-12-17T03:25:29.497437Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}